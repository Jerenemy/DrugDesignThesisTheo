{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b166161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "# %run Synth.ipynb\n",
    "# %run AutodockVina.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed14fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d917ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82effce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleReward(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        self.reward_list = []\n",
    "    \n",
    "    @abstractmethod\n",
    "    def giveReward(self,mol):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def name(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3795ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class test(SingleReward):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        super(test,self).__init__()\n",
    "    def giveReward(self,mol):\n",
    "        return 3    \n",
    "    def name(self):\n",
    "        return \"123\"\n",
    "        \n",
    "testq = test(3)\n",
    "testq.reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3141dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DockReward(SingleReward):\n",
    "    def __init__(self,receptor_path):\n",
    "        self.reward_list = []\n",
    "        self.vinaWrapper = VinaWrapper(receptor_path)\n",
    "        \n",
    "    def name(self):\n",
    "        return \"DockReward\"\n",
    "    \n",
    "    def rescale(self,value):\n",
    "        return (-1*value) / 4\n",
    "    \n",
    "    def giveReward(self,mol):\n",
    "        smile = Chem.MolToSmile(mol)\n",
    "        return self.rescale(self.vinaWrapper.CalculateEnergies())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec674c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscWrapper():\n",
    "    def __init__(self, in_dim, hidden_dim,lr):\n",
    "        self.disc = GraphDiscriminator(in_dim, hidden_dim)\n",
    "        self.optim = Adam(self.disc.parameters(), lr=lr,eps=1e-5, weight_decay=.001)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.disc(x)\n",
    "    \n",
    "    \n",
    "    def train(self,X,y):\n",
    "        pass\n",
    "    \n",
    "    def train_on_batch(self, X, y):\n",
    "        X_batch = dgl.batch(X)\n",
    "        X_pred = self.disc(X_batch)\n",
    "        print (X_pred.shape, y.shape)\n",
    "        loss = nn.BCELoss()(X_pred, y)\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aadd6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscReward(SingleReward):\n",
    "    def __init__(self,path,in_dim,hidden_dim,lr):\n",
    "        self.reward_list = []\n",
    "        self.loss = []\n",
    "        self.discWrapper = DiscWrapper(in_dim,hidden_dim,lr)\n",
    "    \n",
    "    def name(self):\n",
    "        return \"DiscReward\"\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        \n",
    "        self.discWrapper.train_on_batch(X,y)\n",
    "    \n",
    "    def giveReward(self,mol):\n",
    "        graph = mol_to_graph(mol)\n",
    "        return self.discWrapper.disc(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9741ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeSynth_norm(SingleReward):\n",
    "    '''module for calculating SA score while hopefully not incentivizing degenerate solutions'''\n",
    "    def __init__(self,size_importance=1, synth_importance=1):\n",
    "        self.synth_model = sascorer\n",
    "        self.reward_list = []\n",
    "        self.synth_importance = synth_importance\n",
    "        self.size_importance = size_importance\n",
    "        \n",
    "    def name(self):\n",
    "        return (\"SA score with size incentive \")\n",
    "    \n",
    "    def giveReward(self,mol):\n",
    "        Chem.SanitizeMol(mol)\n",
    "        synth_reward_norm = torch.sigmoid(torch.tensor(-(self.synth_model.calculateScore(mol) - 3.5)))#^(1/self.synth_importance)\n",
    "        size_reward_norm = torch.sigmoid(torch.tensor(mol.GetNumAtoms()-20))#^(1/self.size_importance)\n",
    "        \n",
    "        product = synth_reward_norm*size_reward_norm #[0,1]\n",
    "        product_norm = (product*3)-3\n",
    "                \n",
    "        return product_norm\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb22cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeReward(SingleReward):\n",
    "    def __init__(self):\n",
    "        self.reward_list = []\n",
    "        \n",
    "    def name(self):\n",
    "        return \"SizeReward\"\n",
    "    \n",
    "    def rescale(self,value):\n",
    "        return value/4\n",
    "    \n",
    "    def giveReward(self,mol):\n",
    "        return(len(mol.GetAtoms()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5fae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Synthesizability(SingleReward):\n",
    "    '''\n",
    "    reward module for synthesizability\n",
    "    values seem to range from 0-7 with lower being better\n",
    "    rescale to be in [-3,3] with flip\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.synth_model = sascorer\n",
    "        self.reward_list = []\n",
    "       \n",
    "    \n",
    "    def name(self):\n",
    "        return \"SynthReward\"\n",
    "        \n",
    "    \n",
    "    def rescale(self, value):\n",
    "        value = (value - 3.5)*-.9\n",
    "        return value\n",
    "    \n",
    "    def giveReward(self,mol):\n",
    "        smile = Chem.MolToSmiles(mol)\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        reward = self.synth_model.calculateScore(mol)\n",
    "        reward = self.rescale(reward)\n",
    "        self.reward_list.append(reward)\n",
    "        #print(smile,reward)\n",
    "        return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1445d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalRewardModule():\n",
    "    '''reward function handling'''\n",
    "    def __init__(self,writer,r_list):\n",
    "        self.r_list = r_list\n",
    "        self.writer = writer\n",
    "        self.color = ['blue','orange','red','green','yellow']\n",
    "        self.n_iter = 0\n",
    "    \n",
    "    def UpdateTrainingModules(self):\n",
    "        pass\n",
    "\n",
    "    def PlotRewards(self):\n",
    "        for idx,SingleReward in enumerate(self.r_list):\n",
    "            plt.plot(SingleReward.reward_list, label = SingleReward.name(),color = self.color[idx])\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def GiveReward(self,mol):\n",
    "        self.n_iter += 1\n",
    "        mol.UpdatePropertyCache()\n",
    "        rewards = 0\n",
    "        for rewardObject in self.r_list:\n",
    "            reward = rewardObject.giveReward(mol)\n",
    "            self.writer.add_scalar(rewardObject.name(), reward, self.n_iter)\n",
    "            rewards += reward\n",
    "        \n",
    "        return rewards\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f42ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardStatistic(rew, smiles, form):\n",
    "    reward_list = []\n",
    "    for smile in smiles:\n",
    "        mol_rep = smile\n",
    "        if form == 'mol':\n",
    "            try:\n",
    "                mol_rep = Chem.MolFromSmiles(smile)\n",
    "            except:\n",
    "                print(smile)\n",
    "                \n",
    "        try:\n",
    "            reward_list.append(rew.giveReward(mol_rep))\n",
    "        except:\n",
    "            print(smile)\n",
    "    plt.hist(reward_list,17)\n",
    "    plt.show()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
