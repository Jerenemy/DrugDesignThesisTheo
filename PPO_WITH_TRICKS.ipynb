{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3b35ff",
   "metadata": {},
   "source": [
    "# PPO Main\n",
    "## PPO with tricks as well as tool for correct init of model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d02f835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_recursive(m):\n",
    "    \n",
    "    if isinstance(m,nn.Linear):\n",
    "        m.apply(init_weights)\n",
    "    elif isinstance(m,Linear_3):\n",
    "        m.apply(init_weights)\n",
    "    elif isinstance(m, Batch_Edge):\n",
    "        m.NodeEmbed.apply(init_weights)\n",
    "        m.Edges.apply(init_weights)\n",
    "    elif isinstance(m, Spin2):\n",
    "        m.Weight.apply(init_weights_recursive)\n",
    "        m.AddNode.apply(init_weights_recursive)\n",
    "        m.BatchEdge.apply(init_weights_recursive)\n",
    "        m.GGN.linears[0].apply(init_weights)\n",
    "        m.GGN.linears[1].apply(init_weights)\n",
    "        m.GGN.linears[2].apply(init_weights)\n",
    "        m.GGN.linears[3].apply(init_weights)\n",
    "        m.GGN.gru.apply(init_weights)\n",
    "    elif isinstance(m,torch.nn.modules.container.ModuleList):\n",
    "        m[0].apply(init_weights)\n",
    "        m[1].apply(init_weights)\n",
    "        m[2].apply(init_weights)\n",
    "        m[3].apply(init_weights)\n",
    "        \n",
    "    elif isinstance(m,dgl.nn.pytorch.conv.gatedgraphconv.GatedGraphConv):\n",
    "        m.apply(init_weights)\n",
    "    elif isinstance(m,torch.nn.modules.rnn.GRUCell):\n",
    "        m.apply(init_weights)\n",
    "    elif isinstance(m,CriticSqueeze):\n",
    "        m.GGN.apply(init_weights_recursive)\n",
    "        m.Dense.apply(init_weights_recursive)\n",
    "    else:\n",
    "        print(m,type(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf0dea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m): \n",
    "    #print(m)\n",
    "    try:\n",
    "        nn.init.orthogonal_(m.weight.data)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ada4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, RMSprop, SGD\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "class PPO_MAIN:\n",
    "\n",
    "    \n",
    "    def __init__(self, env, batch_size,timesteps_per_batch,clip,a_lr,c_lr,n_updates_per_iteration,max_timesteps_per_episode, gamma, actor, writer):\n",
    "        \"\"\"\n",
    "            Initializes the PPO model, including hyperparameters.\n",
    "\n",
    "            Parameters:\n",
    "                policy_class - the policy class to use for our actor/critic networks.\n",
    "                env - the environment to train on.\n",
    "                hyperparameters - all extra arguments passed into PPO that should be hyperparameters.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "        print(\"correcto\")\n",
    "        self.writer = writer\n",
    "        \n",
    "        # Initialize hyperparameters for training with PPO        \n",
    "        self._init_hyperparameters(batch_size,timesteps_per_batch,clip,a_lr,c_lr,n_updates_per_iteration,max_timesteps_per_episode, gamma)\n",
    "\n",
    "        # Extract environment information\n",
    "        self.env = env\n",
    "        input_dim = env.num_node_feats\n",
    "\n",
    "         # Initialize actor and critic networks\n",
    "        #self.actor = Spin(24,50,4)                                                  # ALG STEP 1\n",
    "        #self.actor = Batch_norm_edge1(24,50,4)\n",
    "        self.critic = CriticSqueeze(input_dim,300)\n",
    "        #self.actor = Spin2(input_dim,300,14)\n",
    "        self.actor = actor\n",
    "        \n",
    "        # Initialize optimizers for actor and critic\n",
    "        \n",
    "        #self.actor_optim = SGD(self.actor.parameters(), lr=.1)\n",
    "        self.actor_optim = Adam(self.actor.parameters(), lr=self.a_lr,eps=1e-5, weight_decay=.001)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=self.c_lr,eps=1e-5, weight_decay=.001)\n",
    "\n",
    "        #self.actor.apply(init_weights_recursive)\n",
    "        self.critic.apply(init_weights_recursive)     \n",
    "     \n",
    "        self.batch_iter = 0\n",
    "    def to_device(self,device):\n",
    "        self.actor.cuda()\n",
    "        self.critic.cuda()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def assignActor(self,new_actor):\n",
    "        self.actor = new_actor\n",
    "    \n",
    "    def _init_hyperparameters(self,batch_size,timesteps_per_batch,clip,a_lr,c_lr,n_updates_per_iteration,max_timesteps_per_episode, gamma):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.timesteps_per_batch = timesteps_per_batch\n",
    "        self.clip = clip\n",
    "        self.a_lr = a_lr\n",
    "        self.max_timesteps_per_episode = max_timesteps_per_episode\n",
    "        self.c_lr = c_lr\n",
    "        self.n_updates_per_iteration = n_updates_per_iteration\n",
    "        self.gamma = gamma\n",
    "     \n",
    "    \n",
    "\n",
    "    def learn(self, total_timesteps):\n",
    "        \"\"\"\n",
    "            Train the actor and critic networks. Here is where the main PPO algorithm resides.\n",
    "\n",
    "            Parameters:\n",
    "                total_timesteps - the total number of timesteps to train for\n",
    "\n",
    "            Return:\n",
    "                None\n",
    "        \"\"\"\n",
    "        t_so_far = 0\n",
    "        while t_so_far < total_timesteps:                                                                       # ALG STEP 2\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens = self.rollout()                     # ALG STEP 3\n",
    "            \n",
    "                \n",
    "            t_so_far += np.sum(batch_lens)\n",
    "\n",
    "            # Increment the number of iterations\n",
    "\n",
    "\n",
    "            # Calculate advantage at k-th iteration\n",
    "            V, _ = self.evaluate(batch_obs, batch_acts, batch_rtgs)\n",
    "            A_k = batch_rtgs.to(device) - V.detach()                                                                       # ALG STEP 5\n",
    "\n",
    "            # One of the only tricks I use that isn't in the pseudocode. Normalizing advantages\n",
    "            # isn't theoretically necessary, but in practice it decreases the variance of \n",
    "            # our advantages and makes convergence much more stable and faster. I added this because\n",
    "            # solving some environments was too unstable without it.\n",
    "            \n",
    "            \n",
    "            \n",
    "            A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "        \n",
    "            # This is the loop where we update our network for some n epochs\n",
    "            \n",
    "            train_data_tuple = []\n",
    "            for i in range(len(batch_obs)):\n",
    "                single_tuple = (batch_obs[i],batch_acts[i],batch_rtgs[i],batch_log_probs[i],A_k[i])\n",
    "                train_data_tuple.append(single_tuple)\n",
    "                \n",
    "   \n",
    "           #self.actor_optim = Adam(self.actor.parameters(), lr=self.a_lr,eps=1e-5)\n",
    "            #self.critic_optim = Adam(self.critic.parameters(), lr=self.c_lr,eps=1e-5)\n",
    "            #self.actor_optim = Adam(self.actor.parameters(), lr=self.a_lr,eps=1e-5)\n",
    "            for _ in range(self.n_updates_per_iteration):                                                       # ALG STEP 6 & 7\n",
    "                random.shuffle(train_data_tuple)\n",
    "                i = 0\n",
    "                batchlet_obs,batchlet_acts,batchlet_rtgs,batchlet_log_probs,A_k_let = zip(*train_data_tuple)\n",
    "                \n",
    "                failed_outer = False\n",
    "                \n",
    "                \n",
    "                \n",
    "                while i < len(batch_obs)-65:\n",
    "                    \n",
    "                    batchlet_obs_slice = batchlet_obs[i : (i+self.batch_size)]\n",
    "                    batchlet_acts_slice = torch.stack(batchlet_acts[i : (i+self.batch_size)],0).to(device)\n",
    "                    batchlet_rtgs_slice = torch.stack(batchlet_rtgs[i : (i+self.batch_size)],0).to(device)\n",
    "                    batchlet_log_probs_slice = torch.stack(batchlet_log_probs[i : (i+self.batch_size)],0).to(device)\n",
    "                    batchlet_A_k_slice = torch.stack(A_k_let[i : (i+self.batch_size)],0).to(device)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                    failed = self.train_on_batch(batchlet_obs_slice, batchlet_acts_slice, batchlet_rtgs_slice,\n",
    "                                        batchlet_log_probs_slice,batchlet_A_k_slice)\n",
    "                    if failed:\n",
    "                        failed_outer = True\n",
    "                        break\n",
    "                    i += self.batch_size\n",
    "                \n",
    "                if failed_outer:\n",
    "                    break\n",
    "                \n",
    "    def train_on_batch(self, batch_obs, batch_acts, batch_rtgs, batch_log_probs, A_k):\n",
    "        V, curr_log_probs = self.evaluate(batch_obs, batch_acts, batch_rtgs)\n",
    "        failed = False\n",
    "        \n",
    "        \n",
    "        \n",
    "        kl_approx = torch.mean(batch_log_probs - curr_log_probs)\n",
    "        self.writer.add_scalar('Approximate KL', kl_approx, self.batch_iter)\n",
    "#         kl_approx = torch.exp(batch_log_probs)*kl_approx\n",
    "#         kl_approx.detach().numpy()\n",
    "#         print(kl_approx)\n",
    "        \n",
    "        if kl_approx>.06:\n",
    "            failed=True\n",
    "        #print(kl_approx, \"kl_approx\")\n",
    "        \n",
    "        ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
    "\n",
    "        # Calculate surrogate losses.\n",
    "        surr1 = ratios * A_k\n",
    "        surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A_k\n",
    "\n",
    "\n",
    "\n",
    "        actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "        critic_loss = nn.MSELoss()(V, batch_rtgs)\n",
    "        #self.critic_loss_over_time.append(critic_loss)\n",
    "\n",
    "        self.writer.add_scalar('Actor Loss', actor_loss, self.batch_iter)\n",
    "        self.writer.add_scalar('Critic Loss', actor_loss, self.batch_iter)\n",
    "        self.batch_iter += 1\n",
    "        #print(critic_loss)\n",
    "#               \n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward(retain_graph = True)#=True)\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), .5)  \n",
    "        self.critic_optim.step()\n",
    "\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), .5)        \n",
    "        self.actor_optim.step()\n",
    "    \n",
    "        return failed\n",
    "    def generate_graphs(self, num_graphs):\n",
    "        graph_list = []\n",
    "        for i in range(num_graphs):\n",
    "            obs = self.env.reset()\n",
    "            for ep_t in range(self.max_timesteps_per_episode):\n",
    "                action, log_prob = self.get_action(obs)\n",
    "                obs, rew, done, reward_dict  = self.env.step(action[0])\n",
    "                if done:\n",
    "                    graph_list.append(selfLoop(self.env.stateSpaceGraph))\n",
    "                    break\n",
    "        print('xx')\n",
    "        \n",
    "        return (graph_list)\n",
    "    \n",
    "    def infer_step(self,verbose=True, start_mol = None):\n",
    "        if start_mol == None:\n",
    "            obs = self.env.reset()\n",
    "        else:\n",
    "            self.StateSpace = Chem.RWMol(start_mol)\n",
    "            \n",
    "            \n",
    "        action, log_prob = self.get_action(obs)\n",
    "        obs, rew, done, reward_dict  = self.env.step(action[0],verbose)\n",
    "        return self.env.StateSpace\n",
    "        \n",
    "    \n",
    "    def inference(self,verbose=False):\n",
    "        reward = 0\n",
    "        obs = self.env.reset()\n",
    "        print(Chem.MolToSmiles(self.env.StateSpace))\n",
    "        for ep_t in range(self.max_timesteps_per_episode):\n",
    "\n",
    "            action, log_prob = self.get_action(obs,True)\n",
    "            print(action)\n",
    "            obs, rew, done, reward_dict  = self.env.step(action[0],verbose)\n",
    "            reward += rew\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return self.env.StateSpace\n",
    "        \n",
    "    def rollout(self):\n",
    "        \"\"\"\n",
    "            Return:\n",
    "                batch_obs - the observations collected this batch. Shape: (number of timesteps, dimension of observation)\n",
    "                batch_acts - the actions collected this batch. Shape: (number of timesteps, dimension of action)\n",
    "                batch_log_probs - the log probabilities of each action taken this batch. Shape: (number of timesteps)\n",
    "                batch_rtgs - the Rewards-To-Go of each timestep in this batch. Shape: (number of timesteps)\n",
    "                batch_lens - the lengths of each episode this batch. Shape: (number of episodes)\n",
    "        \"\"\"\n",
    "        #self.render(50)\n",
    "        \n",
    "        # Batch data. For more details, check function header.\n",
    "        batch_obs = []\n",
    "        batch_acts = []\n",
    "        batch_log_probs = []\n",
    "        batch_rews = []\n",
    "        batch_rtgs = []\n",
    "        batch_lens = []\n",
    "\n",
    "        \n",
    "        # Episodic data. Keeps track of rewards per episode, will get cleared\n",
    "        # upon each new episode\n",
    "        ep_rews = []\n",
    "        \n",
    "        t = 0 # Keeps track of how many timesteps we've run so far this batch\n",
    "\n",
    "        # Keep simulating until we've run more than or equal to specified timesteps per batch\n",
    "        self.batch_reward_plot = 0\n",
    "        \n",
    "        '''Plotting Variables'''\n",
    "        total_reward=0\n",
    "        total_disc_reward = 0\n",
    "        total_hist_reward = 0\n",
    "        num_episodes = 0\n",
    "        \n",
    "        \n",
    "        while t < self.timesteps_per_batch:\n",
    "            \n",
    "            num_episodes += 1\n",
    "            \n",
    "            \n",
    "            ep_rews = [] # rewards collected per episode\n",
    "\n",
    "            # Reset the environment. sNote that obs is short for observation. \n",
    "            obs = self.env.reset()\n",
    "            done = False\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            reward_for_episode = 0 \n",
    "            # Run an episode for a maximum of max_timesteps_per_episode timesteps\n",
    "            for ep_t in range(self.max_timesteps_per_episode):\n",
    "                # If render is specified, render the environment\n",
    "                #self.env.render()\n",
    "                \n",
    "                final = False\n",
    "                if ep_t == self.max_timesteps_per_episode-1: #final step of the generation \n",
    "                    final = True\n",
    "\n",
    "                t += 1 # Increment timesteps ran this batch so far\n",
    "\n",
    "                # Track observations in this batch\n",
    "                batch_obs.append((obs[0].clone(),obs[1],obs[2]))\n",
    "\n",
    "                # Calculate action and make a step in the env. \n",
    "                # Note that rew is short for reward.\n",
    "                action, log_prob = self.get_action(obs)\n",
    "                obs, rew, done, reward_dict  = self.env.step(action[0],final_step = final)\n",
    "                reward_for_episode += rew\n",
    "                \n",
    "                total_disc_reward += reward_dict['model_reward']\n",
    "                total_hist_reward += reward_dict['property_reward']\n",
    "                \n",
    "                \n",
    "                total_reward += rew #track total rewards to get reward per step\n",
    "                # Track recent reward, action, and action log probability\n",
    "                ep_rews.append(rew)\n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "                \n",
    "                # If the environment tells us the episode is terminated, break\n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "            self.batch_reward_plot += reward_for_episode\n",
    "        \n",
    "            #self.ep_rewards_over_time.append(reward_for_episode)\n",
    "            # Track episodic lengths and rewards\n",
    "            #self.ep_rewards_over_time.append(ep_rews)\n",
    "            batch_lens.append(ep_t + 1)\n",
    "            batch_rews.append(ep_rews)\n",
    "\n",
    "        # Reshape data as tensors in the shape specified in function description, before returning\n",
    "        \n",
    "        #batch_obs = torch.tensor(batch_obs, dtype=torch.float)\n",
    "        \n",
    "        '''Logging'''\n",
    "        \n",
    "        \n",
    "        \n",
    "        batch_acts = torch.tensor(batch_acts, dtype=torch.float)\n",
    "        batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float).flatten()\n",
    "        batch_rtgs = self.compute_rtgs(batch_rews)                                                              # ALG STEP 4\n",
    "\n",
    "        # Log the episodic returns and episodic lengths in this batch.\n",
    "\n",
    "\n",
    "        return batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens\n",
    "\n",
    "    def compute_rtgs(self, batch_rews):\n",
    "        \"\"\"\n",
    "            Compute the Reward-To-Go of each timestep in a batch given the rewards.\n",
    "\n",
    "            Parameters:\n",
    "                batch_rews - the rewards in a batch, Shape: (number of episodes, number of timesteps per episode)\n",
    "\n",
    "            Return:\n",
    "                batch_rtgs - the rewards to go, Shape: (number of timesteps in batch)\n",
    "        \"\"\"\n",
    "        # The rewards-to-go (rtg) per episode per batch to return.\n",
    "        # The shape will be (num episodes per batch, num timesteps per episode)\n",
    "        batch_rtgs = []\n",
    "\n",
    "        # Iterate through each episode\n",
    "        for ep_rews in reversed(batch_rews):\n",
    "\n",
    "            discounted_reward = 0 # The discounted reward so far\n",
    "\n",
    "            for rew in reversed(ep_rews):\n",
    "                discounted_reward = rew + discounted_reward * self.gamma\n",
    "                batch_rtgs.insert(0, discounted_reward)\n",
    "\n",
    "        # Convert the rewards-to-go into a tensor\n",
    "        batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float)\n",
    "\n",
    "        return batch_rtgs\n",
    "\n",
    "    def get_action(self, obs, mask_on = False):\n",
    "        \"\"\"\n",
    "            Queries an action from the actor network, should be called from rollout.\n",
    "\n",
    "            Parameters:\n",
    "                obs - the observation at the current timestep\n",
    "\n",
    "            Return:\n",
    "                action - the action to take, as a numpy array\n",
    "                log_prob - the log probability of the selected action in the distribution\n",
    "        \"\"\"\n",
    "        # Query the actor network for a mean action\n",
    "        \n",
    "        #test_out = self.Batch_norm_edge1([obs[0]],[obs[1]],[obs[2]])[0]\n",
    "        \n",
    "        test_spin = self.actor(dgl.add_self_loop(dgl.remove_self_loop(obs[0])),torch.cat([obs[1]],0).to(device),torch.cat([obs[2]],dim = 0),mask = mask_on)\n",
    "        test_dist = Categorical(test_spin)\n",
    "        test_action = test_dist.sample()\n",
    "        test_log_prob = test_dist.log_prob(test_action)\n",
    "    \n",
    "\n",
    "        return test_action.detach().cpu().numpy(), test_log_prob.detach()\n",
    "\n",
    "    def evaluate(self, batch_obs, batch_acts, batch_rtgs):\n",
    "        \"\"\"\n",
    "            Estimate the values of each observation, and the log probs of\n",
    "            each action in the most recent batch with the most recent\n",
    "            iteration of the actor network. Should be called from learn.\n",
    "\n",
    "            Parameters:\n",
    "                batch_obs - the observations from the most recently collected batch as a tensor.\n",
    "                            Shape: (number of timesteps in batch, dimension of observation)\n",
    "                batch_acts - the actions from the most recently collected batch as a tensor.\n",
    "                            Shape: (number of timesteps in batch, dimension of action)\n",
    "                batch_rtgs - the rewards-to-go calculated in the most recently collected\n",
    "                                batch as a tensor. Shape: (number of timesteps in batch)\n",
    "        \"\"\"\n",
    "        \n",
    "        values = []\n",
    "        log_prob_list = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        batch_form_obs = [[],[],[]]\n",
    "        for i in range(len(batch_obs)):\n",
    "            batch_form_obs[0].append(dgl.add_self_loop(dgl.remove_self_loop(batch_obs[i][0])))\n",
    "            batch_form_obs[1].append(batch_obs[i][1].to(device))\n",
    "            batch_form_obs[2].append(batch_obs[i][2].to(device))\n",
    "        \n",
    "        \n",
    "        graph_batch = dgl.batch(batch_form_obs[0])\n",
    "\n",
    "        \n",
    "        \n",
    "        CC = self.critic(graph_batch.to(device),torch.cat(batch_form_obs[1], 0).to(device),torch.cat(batch_form_obs[2], 0).to(device))\n",
    "        A_new = self.actor(dgl.batch(batch_form_obs[0]),torch.cat(batch_form_obs[1], 0).to(device),torch.cat(batch_form_obs[2], 0).to(device))\n",
    "        \n",
    "        \n",
    "        \n",
    "        new_dist = Categorical(A_new)\n",
    "        new_log_prob = new_dist.log_prob(batch_acts.to(device).squeeze())\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        return CC.squeeze(), new_log_prob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533bc5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
