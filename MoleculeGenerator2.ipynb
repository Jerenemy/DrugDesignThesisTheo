{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c605a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import torch\n",
    "from pysmiles import read_smiles\n",
    "import networkx as nx\n",
    "import dgllife\n",
    "from rdkit import Chem\n",
    "import random\n",
    "import dgl.nn.pytorch as dglnn\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "from dgl import DGLGraph\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "\n",
    "import dgl.data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d228bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_recursive(m):\n",
    "    \n",
    "    if isinstance(m,nn.Linear):\n",
    "        m.apply(init_weights)\n",
    "    elif isinstance(m,Linear_3):\n",
    "        m.apply(init_weights)\n",
    "    elif isinstance(m, Batch_Edge):\n",
    "        m.NodeEmbed.apply(init_weights)\n",
    "        m.Edges.apply(init_weights)\n",
    "    elif isinstance(m, BaseLine):\n",
    "        m.AddNode.apply(init_weights_recursive)\n",
    "        m.BatchEdge.apply(init_weights_recursive)\n",
    "        m.GGN.linears[0].apply(init_weights)\n",
    "        m.GGN.linears[1].apply(init_weights)\n",
    "        m.GGN.linears[2].apply(init_weights)\n",
    "        m.GGN.linears[3].apply(init_weights)\n",
    "        m.GGN.gru.apply(init_weights)\n",
    "    elif isinstance(m, Spin2):\n",
    "        m.Weight.apply(init_weights_recursive)\n",
    "        m.AddNode.apply(init_weights_recursive)\n",
    "        m.BatchEdge.apply(init_weights_recursive)\n",
    "        m.GGN.linears[0].apply(init_weights)\n",
    "        m.GGN.linears[1].apply(init_weights)\n",
    "        m.GGN.linears[2].apply(init_weights)\n",
    "        m.GGN.linears[3].apply(init_weights)\n",
    "        m.GGN.gru.apply(init_weights)\n",
    "    elif isinstance(m,torch.nn.modules.container.ModuleList):\n",
    "        m[0].apply(init_weights)\n",
    "        m[1].apply(init_weights)\n",
    "        m[2].apply(init_weights)\n",
    "        m[3].apply(init_weights)\n",
    "        \n",
    "    elif isinstance(m,dgl.nn.pytorch.conv.gatedgraphconv.GatedGraphConv):\n",
    "        m.apply(init_weights)\n",
    "    elif isinstance(m,torch.nn.modules.rnn.GRUCell):\n",
    "        m.apply(init_weights)\n",
    "    elif isinstance(m,CriticSqueeze):\n",
    "        m.GGN.apply(init_weights_recursive)\n",
    "        m.Dense.apply(init_weights_recursive)\n",
    "    else:\n",
    "        print(m,type(m))\n",
    "def init_weights(m): \n",
    "    #print(m)\n",
    "    try:\n",
    "        nn.init.orthogonal_(m.weight.data)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d28c351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selfLoop(graph):\n",
    "    return dgl.add_self_loop(dgl.remove_self_loop(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a560be46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomAtomFeaturizer(mol):\n",
    "    '''\n",
    "    atom type, bond_number, formal charge, chirality, \n",
    "    number of bonded h atoms, hybridization, aromaticity,\n",
    "    atomic mass scaled\n",
    "    \n",
    "    '''\n",
    "    feats = []\n",
    "    \n",
    "    atom_ids = [1, 8, 6, 17, 7, 15, 35, 16, 9, 53]\n",
    "    hybridization_ids = [\"SP\", \"SP2\", \"SP3\"]\n",
    "    for atom in mol.GetAtoms():\n",
    "        \n",
    "        atomic_num = atom.GetAtomicNum() #9 values it seems \n",
    "        id = atom_ids.index(atomic_num)\n",
    "        atomic_num_onehot = np.zeros(10)\n",
    "        atomic_num_onehot[id] = 1\n",
    "        \n",
    "        num_bonds = atom.GetDegree() #tuple, is this the same len and degree?\n",
    "        num_bonds_onehot = np.zeros(4)\n",
    "        num_bonds_onehot[num_bonds-1] = 0\n",
    "        \n",
    "        formal_charge = np.expand_dims(np.asarray(atom.GetFormalCharge()),0) #looks like between 1 and -1\n",
    "        \n",
    "        hybridization = atom.GetHybridization().name # either SP, SP2, SP3\n",
    "        id = hybridization.index(hybridization)\n",
    "        hybridization_onehot = np.zeros(3)\n",
    "        hybridization_onehot[id] = 1\n",
    "        \n",
    "        is_aromatic = np.expand_dims(np.asarray(atom.GetIsAromatic()).astype(int),0) # Bool\n",
    "        \n",
    "        valence = atom.GetTotalValence() #between 1 and 6\n",
    "        valence_onehot = np.zeros(6)\n",
    "        valence_onehot[valence-1] = 1\n",
    "        \n",
    "        mass = np.expand_dims(np.asarray(atom.GetMass())/127,0) # max val of 126.904\n",
    "\n",
    "        \n",
    "        feats.append(np.concatenate((atomic_num_onehot,num_bonds_onehot,formal_charge,\n",
    "                                     hybridization_onehot,is_aromatic,valence_onehot,mass)))\n",
    "        #print(np.asarray(feats).shape)\n",
    "    #print(torch.tensor(feats).shape)\n",
    "    return {'atomic': torch.tensor(feats).float()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb607d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    '''\n",
    "    Wrapper for deepchem batch iterator\n",
    "    '''\n",
    "    def __init__(self,batch_size, disk_dataset,task_ids, atom_featurizer = None, edge_featurizer = None):\n",
    "        \n",
    "        self.disk_dataset = disk_dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.data_iter = disk_dataset.iterbatches(batch_size)\n",
    "        \n",
    "        if edge_featurizer == None:\n",
    "            self.edge_featurizer = self.__base_edge_featurizer\n",
    "        else:\n",
    "            self.edge_featurizer = edge_featurizer\n",
    "        \n",
    "        if atom_featurizer == None:\n",
    "            self.atom_featurizer = self.__base_atom_featurizer\n",
    "        else:\n",
    "            self.atom_featurizer = atom_featurizer\n",
    "            \n",
    "            \n",
    "            \n",
    "    def __complement(self, graph):\n",
    "        '''\n",
    "        Function for finding complement of a graph\n",
    "        to be used down the line for link prediction \n",
    "\n",
    "        complement could be disconnected \n",
    "        '''\n",
    "        u = []\n",
    "        v = []\n",
    "        for i in range(graph.num_nodes()):\n",
    "            for j in range(i+1,graph.num_nodes()):\n",
    "                if not (graph.has_edges_between(i, j)):\n",
    "                    u.append(i)\n",
    "                    v.append(j)\n",
    "\n",
    "        complement = dgl.graph((u+v,v+u), num_nodes=graph.num_nodes())\n",
    "        return complement\n",
    "    \n",
    "        \n",
    "    def __base_atom_featurizer(self, mol):\n",
    "        feats = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            feats.append(atom.GetAtomicNum())\n",
    "            \n",
    "        return {'atomic': torch.tensor(feats).reshape(-1, 1).float()}\n",
    "        \n",
    "    def __my_atom_featurizer(self, mol):\n",
    "        atom_list = ['N','C','O','S']\n",
    "        atom_bond_dict = {'N':[1,0,5], 'C':[2,0,4], 'O':[3,0,6], 'S':[4,0,6]}\n",
    "        feats = []\n",
    "        \n",
    "        def oneHot(number, max_size, index):\n",
    "            OneHot = np.zeros(max_size)\n",
    "            OneHot[number-index] = 1\n",
    "            return OneHot\n",
    "        \n",
    "        for atom in mol.GetAtoms():\n",
    "            atom_type, degree, valence = self.atom_bond_dict[atom.GetSymbol()]\n",
    "            \n",
    "            degree_onehot = self.oneHot(atom.GetExplicitValence(),12,0)\n",
    "            atom_type_onehot = self.oneHot(atom_type,4,1)\n",
    "            valence_onehot = self.oneHot(valence,8,1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            feats.append(degree_onehot,atom_type_onehot,valence)\n",
    "            \n",
    "        \n",
    "        return {'atomic': torch.tensor(feats).float()}\n",
    "        \n",
    "        \n",
    "    def __base_edge_featurizer(self ,mol, add_self_loop = False):\n",
    "        feats = []\n",
    "        bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE,\n",
    "                    Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
    "        for bond in mol.GetBonds():\n",
    "            btype = bond_types.index(bond.GetBondType())\n",
    "            # One bond between atom u and v corresponds to two edges (u, v) and (v, u)\n",
    "            feats.extend([btype, btype])\n",
    "        return {'type': torch.tensor(feats).reshape(-1, 1).float()}    \n",
    "    def produce_batch_smiles(self,smiles):\n",
    "        graphs = []\n",
    "        for smile in smiles:\n",
    "            graph = dgllife.utils.smiles_to_bigraph(smiles[i], node_featurizer=self.atom_featurizer, \n",
    "                                                            edge_featurizer=self.edge_featurizer)\n",
    "            graphs.append(graph)\n",
    "        return graphs\n",
    "    def produce_batch(self):\n",
    "        graphs = []\n",
    "        complements = []\n",
    "        X, labels, something, smiles = self.data_iter.__next__()\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            graph = dgllife.utils.smiles_to_bigraph(smiles[i], node_featurizer=self.atom_featurizer, \n",
    "                                                            edge_featurizer=self.edge_featurizer)\n",
    "            \n",
    "            complements.append(self.__complement(graph))\n",
    "            graphs.append(graph)\n",
    "        #graphs = dgl.batch(graphs)\n",
    "        return (graphs)#, torch.from_numpy(np.asarray(labels).astype('float32'))    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f0146",
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_bond_dict = {'N':[1,0,5], 'C':[2,0,4], 'O':[3,0,6], 'S':[4,0,6],\n",
    "                               'F':[5,0,7], 'Cl' : [6,0,7],'Na':[7,0,7], 'P' : [8,0,5],\n",
    "                               'Br':[9,0,7], 'Si' : [10,0,4],'B':[11,0,3], 'Se' : [12,0,6],\n",
    "                               'K':[13,0,7]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7df96f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'C' in atom_bond_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b056d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol_checker(mol_rep, form = 'MOL'):\n",
    "    atom_bond_dict = {'N':[1,0,5], 'C':[2,0,4], 'O':[3,0,6], 'S':[4,0,6],\n",
    "                               'F':[5,0,7], 'Cl' : [6,0,7],'Na':[7,0,7], 'P' : [8,0,5],\n",
    "                               'Br':[9,0,7], 'Si' : [10,0,4],'B':[11,0,3], 'Se' : [12,0,6],\n",
    "                               'K':[13,0,7]}\n",
    "    \n",
    "    if form == \"SMILES\":\n",
    "        if mol_rep == 'nan':\n",
    "            return False\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(mol_rep)\n",
    "        except:\n",
    "            return False\n",
    "    elif form == \"GRAPH\":\n",
    "        mol = MolFromGraphsAro(mol_rep)\n",
    "    \n",
    "    elif form == 'MOL':\n",
    "        mol = mol_rep\n",
    "        \n",
    "        \n",
    "    \n",
    "    aro_atoms =  mol.GetAromaticAtoms()\n",
    "\n",
    "    if len(aro_atoms)%6 != 0 or not all([aro_atom.GetSymbol() == 'C' for aro_atom in aro_atoms]):\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    for bond in mol.GetBonds():\n",
    "        if bond.GetBondTypeAsDouble() >= 3:\n",
    "            return False\n",
    "        \n",
    "    for atom in mol.GetAtoms():\n",
    "        try:\n",
    "            atom.UpdatePropertyCache()\n",
    "        except:\n",
    "            return False\n",
    "        if (atom.GetSymbol() not in atom_bond_dict) or (8-atom_bond_dict[atom.GetSymbol()][-1] < atom.GetExplicitValence()):\n",
    "            return False\n",
    "    return True\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cb22dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_atoms(mol):\n",
    "    feats = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        feat = np.concatenate()\n",
    "        feats.append(atom.GetAtomicNum())\n",
    "    return {'atomic': torch.tensor(feats).reshape(-1, 1).float()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18721d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def CustomAtomFeaturizer(mol):\n",
    "    '''\n",
    "    atom type, bond_number, formal charge, chirality, \n",
    "    number of bonded h atoms, hybridization, aromaticity,\n",
    "    atomic mass scaled\n",
    "    \n",
    "    '''\n",
    "    feats = []\n",
    "    \n",
    "    atom_bond_dict = {'N':[1,0,5], 'C':[2,0,4], 'O':[3,0,6], 'S':[4,0,6],\n",
    "                               'F':[5,0,7], 'Cl' : [6,0,7],'Na':[7,0,7], 'P' : [8,0,5],\n",
    "                               'Br':[9,0,7], 'Si' : [10,0,4],'B':[11,0,3], 'Se' : [12,0,6],\n",
    "                               'K':[13,0,7]}\n",
    "    hybridization_ids = [\"SP\", \"SP2\", \"SP3\"]\n",
    "    for atom in mol.GetAtoms():\n",
    "        \n",
    "        atomic_num = atom.GetAtomicNum() #9 values it seems \n",
    "        idx = atom_bond_dict[atom.GetSymbol()][0]\n",
    "        \n",
    "        \n",
    "        atomic_num_onehot = np.zeros(14)\n",
    "        atomic_num_onehot[idx] = 1\n",
    "        \n",
    "        num_bonds = atom.GetDegree() #tuple, is this the same len and degree?\n",
    "        num_bonds_onehot = np.zeros(4)\n",
    "        num_bonds_onehot[num_bonds-1] = 0\n",
    "        \n",
    "        formal_charge = np.expand_dims(np.asarray(atom.GetFormalCharge()),0) #looks like between 1 and -1\n",
    "        \n",
    "        hybridization = atom.GetHybridization().name # either SP, SP2, SP3\n",
    "        id = hybridization.index(hybridization)\n",
    "        hybridization_onehot = np.zeros(3)\n",
    "        hybridization_onehot[id] = 1\n",
    "        \n",
    "        is_aromatic = np.expand_dims(np.asarray(atom.GetIsAromatic()).astype(int),0) # Bool\n",
    "        \n",
    "        valence = atom.GetTotalValence() #between 1 and 6\n",
    "        valence_onehot = np.zeros(6)\n",
    "        valence_onehot[valence-1] = 1\n",
    "        \n",
    "        mass = np.expand_dims(np.asarray(atom.GetMass())/127,0) # max val of 126.904\n",
    "\n",
    "        \n",
    "        feats.append(np.concatenate((atomic_num_onehot,num_bonds_onehot,formal_charge,\n",
    "                                     hybridization_onehot,is_aromatic,valence_onehot,mass)))\n",
    "\n",
    "    return {'atomic': torch.tensor(feats).float()}\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff7e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_graph(smiles):\n",
    "    graphs = []\n",
    "    for smile in smiles:\n",
    "        try:\n",
    "            graph = dgllife.utils.smiles_to_bigraph(smile, node_featurizer=my_atom_featurizer, edge_featurizer=base_edge_featurizer)\n",
    "            graph = dgl.add_self_loop(dgl.remove_self_loop(graph))\n",
    "            graphs.append(graph)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8672e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol_to_graph(mol):\n",
    "    graph = dgllife.utils.mol_to_bigraph(mol, node_featurizer=my_atom_featurizer, edge_featurizer=base_edge_featurizer,\n",
    "                                        canonical_atom_order=False)\n",
    "    graph = dgl.add_self_loop(dgl.remove_self_loop(graph))\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5a8f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_graph(graph):\n",
    "    try:\n",
    "        Mol = MolFromGraphsAro(graph)\n",
    "        graph = mol_to_graph(Mol)\n",
    "        selfLoop(graph)\n",
    "        return graph\n",
    "    except:\n",
    "        return smiles_to_graph(['C-C'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64189e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_edge_featurizer(mol, add_self_loop = False):\n",
    "    feats = []\n",
    "    bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE,\n",
    "                 Chem.rdchem.BondType.AROMATIC,Chem. rdchem.BondType.TRIPLE]\n",
    "    for bond in mol.GetBonds():\n",
    "        btype = bond_types.index(bond.GetBondType())+1\n",
    "        # One bond between atom u and v corresponds to two edges (u, v) and (v, u)\n",
    "        feats.extend([btype, btype])\n",
    "    return {'type': torch.tensor(feats).reshape(-1, 1).float()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b64a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_atom_featurizer2(mol):\n",
    "    atom_list = ['N','C','O','S','F','Cl','Na','P','Br','Si','B','Se','K']\n",
    "    atom_bond_dict = {'N':[1,0,5], 'C':[2,0,4], 'O':[3,0,6], 'S':[4,0,6],\n",
    "                               'F':[5,0,7], 'Cl' : [6,0,7],'Na':[7,0,7], 'P' : [8,0,5],\n",
    "                               'Br':[9,0,7], 'Si' : [10,0,4],'B':[11,0,3], 'Se' : [12,0,6],\n",
    "                               'K':[13,0,7]}\n",
    "    feats = []\n",
    "\n",
    "    def oneHot(number, max_size, index):\n",
    "        OneHot = np.zeros(max_size)\n",
    "        OneHot[number-index] = 1\n",
    "        return OneHot\n",
    "\n",
    "    for atom in mol.GetAtoms():\n",
    "        \n",
    "        atom_type, degree, valence = atom_bond_dict[atom.GetSymbol()]\n",
    "        degree_onehot = oneHot(atom.GetExplicitValence(),12,0)\n",
    "        atom_type_onehot = oneHot(atom_type,14,1)\n",
    "        valence_onehot = oneHot(valence,8,1)\n",
    "\n",
    "\n",
    "        \n",
    "        feats.append(np.concatenate((degree_onehot,atom_type_onehot,valence_onehot)))\n",
    "\n",
    "\n",
    "    return {'atomic': torch.tensor(feats).float()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953a8d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_atom_featurizer(mol):\n",
    "    atom_list = ['N','C','O','S','F','Cl','Na','P','Br','Si','B','Se','K']\n",
    "    atom_bond_dict = {'N':[1,0,5], 'C':[2,0,4], 'O':[3,0,6], 'S':[4,0,6],\n",
    "                               'F':[5,0,7], 'Cl' : [6,0,7],'Na':[7,0,7], 'P' : [8,0,5],\n",
    "                               'Br':[9,0,7], 'Si' : [10,0,4],'B':[11,0,3], 'Se' : [12,0,6],\n",
    "                               'K':[13,0,7]}\n",
    "    feats = []\n",
    "\n",
    "    def oneHot(number, max_size, index):\n",
    "        OneHot = np.zeros(max_size)\n",
    "        OneHot[number-index] = 1\n",
    "        return OneHot\n",
    "\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom.UpdatePropertyCache()\n",
    "        atom_type, degree, valence = atom_bond_dict[atom.GetSymbol()]\n",
    "        #atom.calcExplicitValence()\n",
    "        degree_onehot = oneHot(atom.GetExplicitValence(),12,0)\n",
    "        atom_type_onehot = oneHot(atom_type,14,1)\n",
    "        valence_onehot = oneHot(valence,8,1)\n",
    "\n",
    "\n",
    "        \n",
    "        feats.append(np.concatenate((degree_onehot,atom_type_onehot,valence_onehot)))\n",
    "\n",
    "\n",
    "    return {'atomic': torch.tensor(feats).float()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e21e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_atom_featurizer(mol):\n",
    "    feats = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        feats.append([1,3])\n",
    "    return {'atomic': torch.tensor(feats).float()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90248685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atom_type_from_tensor(tensor):\n",
    "    atom_list =['N','C','O','S','F','Cl','Na','P','Br','Si','B','Se','K']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc27175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomAtomFeaturizer_full(mol):\n",
    "    '''\n",
    "    atom type, bond_number, formal charge, chirality, \n",
    "    number of bonded h atoms, hybridization, aromaticity,\n",
    "    atomic mass scaled\n",
    "    \n",
    "    '''\n",
    "    feats = []\n",
    "    \n",
    "    atom_bond_dict = {'N':[1,0,5], 'C':[2,0,4], 'O':[3,0,6], 'S':[4,0,6],\n",
    "                               'F':[5,0,7], 'Cl' : [6,0,7],'Na':[7,0,7], 'P' : [8,0,5],\n",
    "                               'Br':[9,0,7], 'Si' : [10,0,4],'B':[11,0,5], 'Se' : [12,0,6],\n",
    "                               'K':[13,0,7]}\n",
    "    \n",
    "    hybridization_ids = [\"SP\", \"SP2\", \"SP3\"]\n",
    "    \n",
    "    Chem.SetHybridization(mol)\n",
    "    \n",
    "    for atom in mol.GetAtoms():\n",
    "        atom.UpdatePropertyCache()\n",
    "        atom_symbol = atom.GetSymbol()\n",
    "        atom_idx = atom_bond_dict[atom_symbol][0]-1\n",
    "        \n",
    "        atom_oh = np.zeros(15)\n",
    "        atom_oh[atom_idx] = 1\n",
    "        \n",
    "        max_valence = atom_bond_dict[atom_symbol][-1]\n",
    "        max_valence_oh = np.zeros(8)\n",
    "        max_valence_oh[max_valence] = 1\n",
    "        \n",
    "        degree = dgllife.utils.atom_degree_one_hot(atom)\n",
    "                \n",
    "        hybridization = dgllife.utils.atom_hybridization_one_hot(atom)\n",
    "        \n",
    "        is_aromatic = np.expand_dims(np.asarray(atom.GetIsAromatic()).astype(int),0) # Bool\n",
    "        \n",
    "        exp_valence = dgllife.utils.atom_explicit_valence_one_hot(atom)\n",
    "        imp_valence = dgllife.utils.atom_implicit_valence_one_hot(atom)\n",
    "        \n",
    "        mass = np.expand_dims(np.asarray(atom.GetMass())/127,0) # max val of 126.904\n",
    "        \n",
    "        feat = np.concatenate((atom_oh,max_valence_oh,degree,\n",
    "                                     hybridization,is_aromatic,exp_valence,imp_valence,mass))\n",
    "        \n",
    "        feats.append(feat)\n",
    "    return {'atomic': torch.tensor(feats).float()}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcbfb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_featurizer_full(mol, add_self_loop = False):\n",
    "        feats = []\n",
    "        bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE,\n",
    "                    Chem.rdchem.BondType.AROMATIC]\n",
    "        for bond in mol.GetBonds():\n",
    "            btype = bond_types.index(bond.GetBondType())+1\n",
    "            # One bond between atom u and v corresponds to two edges (u, v) and (v, u)\n",
    "            feats.extend([btype, btype])\n",
    "        return {'type': torch.tensor(feats).reshape(-1, 1).float()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35125d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol_to_graph_full(mol):\n",
    "    graph = dgllife.utils.mol_to_bigraph(mol, node_featurizer=my_atom_featurizer_full, edge_featurizer=base_edge_featurizer_full,\n",
    "                                        canonical_atom_order=False)\n",
    "    graph = dgl.add_self_loop(dgl.remove_self_loop(graph))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5785b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddEdge(nn.Module):\n",
    "    '''\n",
    "    Add edge between last generated node and \n",
    "    a molecule in the already made graph\n",
    "    \n",
    "    returns a matrix which is 2 * num_nodes\n",
    "    \n",
    "    \n",
    "    arohfarflsdjk have to fix\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(AddEdge, self).__init__()\n",
    "        self.GGC_1 = dgl.nn.GatedGraphConv(in_dim,hidden_dim,5,4)\n",
    "        self.GGC_2 = dgl.nn.GatedGraphConv(in_dim,hidden_dim,5,4)\n",
    "        self.NodeEmbed = nn.Linear(in_dim,hidden_dim)\n",
    "            \n",
    "    def forward(self,g,h,e, new_node):\n",
    "        #expects new_node.shape = (1, node_feats)\n",
    "        new_node_embed = self.NodeEmbed(new_node)\n",
    "        new_node_embed = torch.transpose(new_node_embed,-1,0)\n",
    "        \n",
    "        \n",
    "        h1 = torch.tanh(self.GGC_1(g,h,e))\n",
    "        h2 = torch.tanh(self.GGC_2(g,h,e))\n",
    "        \n",
    "        score_1 = torch.softmax(torch.matmul(h1,new_node_embed),dim = 0)\n",
    "        score_2 = torch.softmax(torch.matmul(h2,new_node_embed),dim = 0)\n",
    "        \n",
    "        \n",
    "        score_1 = torch.transpose(score_1,1,0) \n",
    "        score_2 = torch.transpose(score_2,1,0) \n",
    "        \n",
    "        \n",
    "        \n",
    "        return torch.cat((score_1,score_2),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7ee18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNode(nn.Module):\n",
    "    '''\n",
    "    class for adding a node and connecting it\n",
    "    first pick new node, then figure out where it goes\n",
    "    Rewording: GraphClassification, NodeClassificiationish\n",
    "    Could just condition node classification on some embedding of the chosen atom\n",
    "    '''\n",
    "    def __init__(self,in_dim, hidden_dim, num_atom):\n",
    "        super(AddNode, self).__init__()\n",
    "        self.GGC_NewNode = dgl.nn.GatedGraphConv(in_dim,hidden_dim,5,4)\n",
    "        self.Pool = dgl.nn.pytorch.glob.GlobalAttentionPooling(nn.Linear(hidden_dim,20))\n",
    "        self.Dense = nn.Linear(hidden_dim, 50)\n",
    "        self.NodeProposal = nn.Linear(50,num_atom)\n",
    "        \n",
    "        \n",
    "    def forward(self,g,h,e):\n",
    "        h = F.relu(self.GGC_NewNode(g,h,e))\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            hg = dgl.mean_nodes(g, 'h')\n",
    "        out = torch.relu(self.Dense(hg))\n",
    "        out = self.NodeProposal(out)\n",
    "        return hg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e1af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_3(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim,out_dim):\n",
    "        super(Linear_3, self).__init__()\n",
    "        self.Dense1 = nn.Linear(in_dim,hidden_dim)\n",
    "        self.Dense2 = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.Dense3 = nn.Linear(hidden_dim,out_dim)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = (torch.tanh(self.Dense1(inputs)))\n",
    "        out = (torch.tanh(self.Dense2(out)))\n",
    "        out = self.Dense3(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f94a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_3_bn(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim,out_dim):\n",
    "        super(Linear_3_bn, self).__init__()\n",
    "        self.Dense1 = nn.Linear(in_dim,hidden_dim)\n",
    "        self.Dense2 = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.Dense3 = nn.Linear(hidden_dim,out_dim)\n",
    "        \n",
    "        self.norm1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.norm2 = nn.BatchNorm1d(hidden_dim)\n",
    "    def forward(self, inputs):\n",
    "        out = self.norm1(torch.relu(self.Dense1(inputs)))\n",
    "        out = self.norm2(torch.relu(self.Dense2(out)))\n",
    "        out = self.Dense3(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleGraphEdge(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(SingleGraphEdge, self).__init__()\n",
    "        self.NodeEmbed = nn.Linear(in_dim, hidden_dim)\n",
    "        self.GGN = dgl.nn.GatedGraphConv(in_dim,hidden_dim,5,4)\n",
    "        self.Edges = Linear_3(hidden_dim*2,hidden_dim*2,2)\n",
    "\n",
    "        \n",
    "    def forward(self, graph, last_node):\n",
    "        h = F.relu(self.GGN(graph,graph.ndata['atomic'],graph.edata['type'].squeeze()))\n",
    "        nodeEmbed = self.NodeEmbed(last_node)\n",
    "        new_node_array = nodeEmbed.repeat(graph.num_nodes(),1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        stack = torch.cat((h,new_node_array),dim = 1)\n",
    "        edges = self.Edges(stack) \n",
    "        edges = torch.reshape(edges, (1,graph.num_nodes()*2))\n",
    "\n",
    "\n",
    "        return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb46bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spin(nn.Module):\n",
    "    '''\n",
    "    Sharing network for message passing, we can then bulk up the dense layers\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim, num_nodes):\n",
    "        super(Spin, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.GGN = dgl.nn.GatedGraphConv(in_dim,hidden_dim,5,4)\n",
    "        \n",
    "        self.Weight = Linear_3(hidden_dim*2, hidden_dim*2, 3)\n",
    "        self.AddNode = Linear_3(hidden_dim, hidden_dim*2,num_nodes)\n",
    "        self.NodeEmbed = nn.Linear(in_dim, hidden_dim)\n",
    "        self.Edges = Linear_3(hidden_dim*2,hidden_dim*2,2)\n",
    "        \n",
    "        \n",
    "    def forward(self, graph, last_action_node, last_node):\n",
    "        \n",
    "        h = F.relu(self.GGN(graph,graph.ndata['atomic'],graph.edata['type']))\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            hg = dgl.mean_nodes(graph, 'h')\n",
    "        \n",
    "        last_action_node = last_action_node.repeat(1,self.hidden_dim)\n",
    "        #print(last_action_node.shape)\n",
    "        # yay forgot to include infor about last_action_node\n",
    "        catted = torch.cat((hg,last_action_node),dim =1 )\n",
    "        weights = self.Weight(catted)\n",
    "        addNode = self.AddNode(hg)\n",
    "        \n",
    "        nodeEmbed = self.NodeEmbed(last_node)\n",
    "        new_node_array = nodeEmbed.repeat(graph.num_nodes(),1)\n",
    "        \n",
    "        \n",
    "        stack = torch.cat((h,new_node_array),dim = 1)\n",
    "        edges = self.Edges(stack) \n",
    "        edges = torch.reshape(edges, (1,graph.num_nodes()*2))\n",
    "        \n",
    "        addNode = addNode * weights[0][0]\n",
    "        addEdge = edges * weights[0][1]\n",
    "        terminate = torch.unsqueeze(torch.unsqueeze(weights[0][2], dim=0), dim = 0)\n",
    "        \n",
    "        \n",
    "        return torch.softmax((torch.cat((terminate,addNode,addEdge),dim = 1)),dim = 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "from torch.distributions import Categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b3e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeTesting(nn.Module):\n",
    "    '''\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(EdgeTesting, self).__init__()\n",
    "        self.GGN = dgl.nn.GatedGraphConv(in_dim,hidden_dim,5,4)\n",
    "\n",
    "        self.NodeEmbed = nn.Linear(in_dim, hidden_dim)\n",
    "        self.Edges = Linear_3(hidden_dim*2,hidden_dim*2,2)\n",
    "        \n",
    "        \n",
    "    def forward(self, graph, last_node_batch):\n",
    "        '''\n",
    "        graph is actually a batch of graphs where len(graph.batch_num_nodes()) = len(last_node_batch)\n",
    "        returns just a list of edges so its unsplit up rn, bbut     \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        h = F.relu(self.GGN(graph,graph.ndata['atomic'],graph.edata['type']))\n",
    "        \n",
    "        \n",
    "        node_embed_batch = self.NodeEmbed(last_node_batch)\n",
    "        batch_node_stacks = [] #holds number of graphs of each node embedding stacked to match node number per graph\n",
    "                               #so its first dimension is the batch size, and the second 'dimension' is the number of nodes per grah\n",
    "        \n",
    "        '''looping over graphs'''\n",
    "        for i in range(last_node_batch.shape[0]):\n",
    "            #curr_stack = torch.cat(last_node)\n",
    "            batch_node_stacks.append(node_embed_batch[i].repeat(graph.batch_num_nodes()[i],1))\n",
    "        \n",
    "                                    \n",
    "        batch_node_stacks = torch.cat(batch_node_stacks, dim = 0)\n",
    "        \n",
    "        stack = torch.cat((h,batch_node_stacks),dim = 1)\n",
    "        edges = self.Edges(stack)\n",
    "        \n",
    "        return edges\n",
    "        return torch.softmax((torch.cat((terminate,addNode,addEdge),dim = 1)),dim = 1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f154de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weighting(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(Weighting, self).__init__()\n",
    "        self.GGN = dgl.nn.GatedGraphConv(in_dim,hidden_dim,5,4)\n",
    "        self.Dense = nn.Linear(hidden_dim, 50)\n",
    "        self.Prediction = nn.Linear(50,3)\n",
    "    def forward(self, graph, last_action_node):\n",
    "        g, h, e = graph, graph.ndata['atomic'], graph.edata['type']\n",
    "        h = F.relu(self.GGN(graph, graph.ndata['atomic'], graph.edata['type']))        \n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            hg = dgl.mean_nodes(graph, 'h')\n",
    "        out = F.relu(self.Dense(hg))\n",
    "        return torch.softmax(self.Prediction(out), dim = 1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32360cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeGenerator(nn.Module):\n",
    "    '''\n",
    "    Molecular Generation Agent\n",
    "    Observation s = (current graph, node_added)\n",
    "    \n",
    "    Returns (terminate, node, edges)   \n",
    "    '''\n",
    "    def __init__(self, in_dim, hidden_dim, num_atoms):\n",
    "        super(MoleculeGenerator, self).__init__()\n",
    "        self.policyNetwork = Weighting(in_dim,hidden_dim)\n",
    "        self.AddNode = AddNode(in_dim,hidden_dim,num_atoms) \n",
    "        self.AddEdge = AddEdge(in_dim,hidden_dim)    \n",
    "        \n",
    "    '''\n",
    "    Produces action probability distribution as well as distribution over that action\n",
    "    '''\n",
    "    def forward(self, graph, last_action_node, last_node):\n",
    "        \n",
    "        g, h, e = graph, graph.ndata['atomic'], graph.edata['type']\n",
    "        \n",
    "        weights = self.policyNetwork(graph,last_action_node)\n",
    "        addNode = self.AddNode(g,h,e) * weights[0][0]\n",
    "        addEdge = self.AddEdge(g,h,e, last_node) * weights[0][1]\n",
    "        terminate = torch.unsqueeze(torch.unsqueeze(weights[0][2], dim=0), dim = 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return torch.sigmoid(torch.cat((terminate,addNode,addEdge),dim = 1))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a96caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDiscriminator(nn.Module):\n",
    "    '''\n",
    "    Critic class for advantage estimation \n",
    "    '''\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(GraphDiscriminator, self).__init__()\n",
    "        self.GGN = dgl.nn.GatedGraphConv(in_dim,hidden_dim,5,5)\n",
    "        self.Dense = Linear_3(hidden_dim,hidden_dim,1)\n",
    "    def forward(self, graph):\n",
    "        #print(graph.adj())\n",
    "        h = F.relu(self.GGN(graph, graph.ndata['atomic'], graph.edata['type'].squeeze()))\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            hg = dgl.mean_nodes(graph, 'h')\n",
    "        torch.nn.Dropout()(hg)\n",
    "        out = torch.sigmoid(self.Dense(hg))\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef84dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticSqueeze(nn.Module):\n",
    "    '''\n",
    "    Critic class for advantage estimation \n",
    "    '''\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(CriticSqueeze, self).__init__()\n",
    "        self.GGN = dgl.nn.GatedGraphConv(in_dim,hidden_dim,5,4)\n",
    "        self.Dense = Linear_3(hidden_dim+1+in_dim,hidden_dim,1)\n",
    "    def forward(self, graph ,last_action_node,last_node):\n",
    "        #print(graph.adj())\n",
    "        h = F.relu(self.GGN(graph, graph.ndata['atomic'], graph.edata['type'].squeeze()))\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            hg = dgl.mean_nodes(graph, 'h')\n",
    "        cat = torch.cat((hg,last_action_node,last_node),dim = 1)\n",
    "        out = self.Dense(cat)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3387793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    '''\n",
    "    Critic class for advantage estimation \n",
    "    '''\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.GGN = dgl.nn.GatedGraphConv(in_dim,hidden_dim,5,4)\n",
    "        self.Dense = Linear_3(hidden_dim+1+in_dim,hidden_dim,1)\n",
    "    def forward(self, graph ,last_action_node,last_node):\n",
    "        #print(graph.adj())\n",
    "        h = F.relu(self.GGN(graph, graph.ndata['atomic'], graph.edata['type']))\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            hg = dgl.mean_nodes(graph, 'h')\n",
    "        cat = torch.cat((hg,last_action_node,last_node),dim = 1)\n",
    "        out = self.Dense(cat)\n",
    "        return out\n",
    "    \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d819da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticTest(nn.Module):\n",
    "    '''\n",
    "    Critic class for advantage estimation \n",
    "    '''\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(CriticTest, self).__init__()\n",
    "        self.GGN = dgl.nn.GatedGraphConv(in_dim,hidden_dim,5,2)\n",
    "        self.Dense = nn.Linear(hidden_dim,1)\n",
    "    def forward(self, graph):\n",
    "        #print(graph.adj())\n",
    "        h = F.relu(self.GGN(graph, graph.ndata['atomic'], graph.edata['type']))\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            hg = dgl.mean_nodes(graph, 'h')\n",
    "        out = self.Dense(hg)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea5637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticBN(nn.Module):\n",
    "    '''\n",
    "    Critic class for advantage estimation \n",
    "    '''\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(CriticBN, self).__init__()\n",
    "        self.GGN = dgl.nn.GatedGraphConv(in_dim,hidden_dim,5,4)\n",
    "        self.Dense = Linear_3_bn(hidden_dim+1+in_dim,hidden_dim,1)\n",
    "    def forward(self, graph,last_action_node,last_node):\n",
    "        h = F.relu(self.GGN(graph, graph.ndata['atomic'], graph.edata['type']))\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            hg = dgl.mean_nodes(graph, 'h')\n",
    "        cat = torch.cat((hg,last_action_node,last_node),dim = 1)\n",
    "        out = self.Dense(cat)\n",
    "        return out\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e66066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatToAtom(feat):\n",
    "    atom_list = ['N','C','O','S']\n",
    "    atom_type_slice = feat[12:12+4]\n",
    "        \n",
    "    atom_type_idx = np.where(atom_type_slice.cpu()==1)\n",
    "    atom_type_idx = atom_type_idx[0][0]\n",
    "    atom_type = atom_list[atom_type_idx]\n",
    "    \n",
    "    return atom_type\n",
    "\n",
    "def MolFromGraphs(graph):\n",
    "\n",
    "    # create empty editable mol object\n",
    "    feat_list = graph.ndata['atomic']\n",
    "    \n",
    "    \n",
    "    \n",
    "    node_list = []\n",
    "    for feat in range(feat_list.shape[0]):\n",
    "        node_list.append(FeatToAtom(feat_list[feat]))\n",
    "\n",
    "    mol = Chem.RWMol()\n",
    "\n",
    "    # add atoms to mol and keep track of index\n",
    "    node_to_idx = {}\n",
    "    for i in range(len(node_list)):\n",
    "        a = Chem.Atom(node_list[i])\n",
    "        molIdx = mol.AddAtom(a)\n",
    "        node_to_idx[i] = molIdx\n",
    "\n",
    "    for u in range(len(node_list)-1):\n",
    "        for v in range(u+1, len(node_list)):\n",
    "            if graph.has_edges_between(u,v):\n",
    "                bond_type = int(graph.edges[u, v][0]['type'].cpu().numpy()[0][0])\n",
    "                if bond_type == 1:\n",
    "                    bond = Chem.rdchem.BondType.SINGLE\n",
    "                elif bond_type == 2:\n",
    "                    bond = Chem.rdchem.BondType.DOUBLE\n",
    "                else:\n",
    "                    print(\"graelfsdjhkarg\")\n",
    "                mol.AddBond(u,v,bond)\n",
    "    mol = mol.GetMol()            \n",
    "\n",
    "    return mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9132391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatToAtomAro(feat):\n",
    "    atom_list = ['N','C','O','S','F','Cl','Na','P','Br','Si','B','Se','K']\n",
    "    atom_type_slice = feat[12:12+13]\n",
    "        \n",
    "    atom_type_idx = np.where(atom_type_slice.cpu()==1)\n",
    "    atom_type_idx = atom_type_idx[0][0]\n",
    "    atom_type = atom_list[atom_type_idx]\n",
    "    \n",
    "    return atom_type\n",
    "\n",
    "def MolFromGraphsAro(graph):\n",
    "\n",
    "    # create empty editable mol object\n",
    "    feat_list = graph.ndata['atomic']\n",
    "    \n",
    "    \n",
    "    \n",
    "    node_list = []\n",
    "    for feat in range(feat_list.shape[0]):\n",
    "        node_list.append(FeatToAtomAro(feat_list[feat]))\n",
    "\n",
    "    mol = Chem.RWMol()\n",
    "\n",
    "    # add atoms to mol and keep track of index\n",
    "    node_to_idx = {}\n",
    "    for i in range(len(node_list)):\n",
    "        a = Chem.Atom(node_list[i])\n",
    "        molIdx = mol.AddAtom(a)\n",
    "        node_to_idx[i] = molIdx\n",
    "\n",
    "    for u in range(len(node_list)-1):\n",
    "        for v in range(u+1, len(node_list)):\n",
    "            if graph.has_edges_between(u,v):\n",
    "                bond_type = int(graph.edges[u, v][0]['type'].cpu().numpy()[0][0])\n",
    "                if bond_type == 1:\n",
    "                    bond = Chem.rdchem.BondType.SINGLE\n",
    "                elif bond_type == 2:\n",
    "                    bond = Chem.rdchem.BondType.DOUBLE\n",
    "                elif bond_type == 3:\n",
    "                    bond = Chem.rdchem.BondType.AROMATIC\n",
    "                else:\n",
    "                    print(\"graelfsdjhkarg\",bond_type,\"asdf\")\n",
    "                mol.AddBond(u,v,bond)\n",
    "    mol = mol.GetMol()            \n",
    "\n",
    "    return mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096cfa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Any_Edge(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(Any_Edge, self).__init__()\n",
    "        self.Edges = Linear_3(hidden_dim,hidden_dim*2,2)\n",
    "        self.in_dim = in_dim\n",
    "        \n",
    "    def forward(self, graphs, h):\n",
    "        '''\n",
    "        graph is actually a batch of graphs where len(graph.batch_num_nodes()) = len(last_node_batch)\n",
    "        returns just a list of edges so its unsplit up rn, bbut     \n",
    "        '''\n",
    "        edges_per_graph = []\n",
    "        \n",
    "        batch_node_stacks = [] #holds number of graphs of each node embedding stacked to match node number per graph\n",
    "                               #so its first dimension is the batch size, and the second 'dimension' is the number of nodes per grah\n",
    "        \n",
    "        nodes_repeat = []\n",
    "        nodes_stack = []\n",
    "        \n",
    "        graphs_batch_num_nodes = graphs.batch_num_nodes()\n",
    "        \n",
    "        tick = 0\n",
    "        for graph_idx in range(len(graphs_batch_num_nodes)):\n",
    "            \n",
    "            graph_nodes = h[tick:tick+graphs_batch_num_nodes[graph_idx]]\n",
    "            \n",
    "            graph_nodes_repeat = graph_nodes.repeat(graphs_batch_num_nodes[graph_idx],1)\n",
    "            graph_node_stack = graph_nodes.repeat_interleave(graphs_batch_num_nodes[graph_idx],0)\n",
    "            \n",
    "            \n",
    "            nodes_repeat.append(graph_nodes_repeat)\n",
    "            nodes_stack.append(graph_node_stack)\n",
    "            \n",
    "            tick += graphs_batch_num_nodes[graph_idx]\n",
    "        \n",
    "\n",
    "        \n",
    "                                    \n",
    "        nodes_repeat = torch.cat(nodes_repeat, dim = 0)\n",
    "        nodes_stack = torch.cat(nodes_stack, dim = 0)\n",
    "        \n",
    "        \n",
    "        dots = nodes_repeat*nodes_stack\n",
    "        edges = self.Edges(dots)\n",
    "        \n",
    "        pp_graph = []\n",
    "        tick = 0\n",
    "        \n",
    "        for graph_num_nodes in (graphs_batch_num_nodes):\n",
    "            graph_e_pred = edges[tick:tick+(graph_num_nodes**2)]\n",
    "\n",
    "            pp_graph.append(graph_e_pred.flatten())\n",
    "            tick += (graph_num_nodes**2)\n",
    "            \n",
    "        return pad_sequence(pp_graph, batch_first = True, padding_value=-10000).flatten(1,-1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f877c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spin3(nn.Module):\n",
    "    '''\n",
    "    Sharing network for message passing, we can then bulk up the dense layers\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim, num_nodes):\n",
    "        super(Spin3, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.GGN = dgl.nn.GatedGraphConv(in_dim,hidden_dim,4,4)\n",
    "        \n",
    "        self.Weight = Linear_3(hidden_dim*2, hidden_dim*2, 3)\n",
    "        self.AddNode = Linear_3(hidden_dim, hidden_dim*2,num_nodes)\n",
    "        self.AnyEdge = Any_Edge(in_dim,hidden_dim)\n",
    "        \n",
    "    def forward(self, graph, last_action_node, softmax = True):\n",
    "#         print(type(graph),\"type)graph\")\n",
    "        out = []\n",
    "        h = F.relu(self.GGN(graph,graph.ndata['atomic'],graph.edata['type'].squeeze()))\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            hg = dgl.mean_nodes(graph, 'h')\n",
    "        \n",
    "        \n",
    "                \n",
    "        \n",
    "        #might need fixing here      \n",
    "        last_action_node = last_action_node.repeat(1,self.hidden_dim)\n",
    "        catted = torch.cat((hg,last_action_node),dim =1 )\n",
    "        #weights = torch.softmax(self.Weight(catted),dim = 1)\n",
    "        #weights = (self.Weight(catted))\n",
    "        #weights = torch.sigmoid(self.Weight(catted))\n",
    "        #print(weights[0:6])\n",
    "        #weights = [[.3],[.3],[.3]]\n",
    "        \n",
    "        addNode = self.AddNode(hg)\n",
    "        edges = self.AnyEdge(graph,h)\n",
    "        #print(edges[0])\n",
    "        \n",
    "        \n",
    "        addNode = addNode * weights[0][0]\n",
    "        edges = edges * weights[0][1]\n",
    "        #print(edges[0])\n",
    "        terminate = weights[:,2:3]\n",
    "        \n",
    "        \n",
    "        out = torch.cat((terminate,addNode,edges),dim = 1)\n",
    "        if softmax == False:\n",
    "            return out\n",
    "        \n",
    "        return torch.softmax((torch.cat((terminate,addNode,edges),dim = 1)),dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e403524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLine(nn.Module):\n",
    "    '''Crappy base line to check improvements against'''\n",
    "    def __init__(self,in_dim,hidden_dim, num_nodes):\n",
    "        super(BaseLine, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.num_nodes = num_nodes\n",
    "        self.GGN = dgl.nn.GatedGraphConv(in_dim, hidden_dim,5,4)\n",
    "        self.AddNode = Linear_3(hidden_dim,hidden_dim,num_nodes)\n",
    "        self.BatchEdge = Batch_Edge(in_dim,hidden_dim)\n",
    "    \n",
    "    def forward(self,graph,last_action_node, last_node, mask = False, softmax = True):\n",
    "        out = []\n",
    "        h = F.relu(self.GGN(graph,graph.ndata['atomic'],graph.edata['type'].squeeze()))\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            hg = dgl.mean_nodes(graph, 'h') \n",
    "            \n",
    "        \n",
    "        addNode = self.AddNode(hg)\n",
    "\n",
    "        if mask:\n",
    "            node_mask = torch.unsqueeze(torch.cat((torch.zeros(1), torch.ones(self.num_nodes-1))),dim=0).to(device)\n",
    "            mask = last_action_node*(node_mask*-100000)\n",
    "            addNode += mask\n",
    "        \n",
    "        \n",
    "        edges = self.BatchEdge(graph,last_node,h)     \n",
    "        out = torch.cat((addNode,edges), dim = 1)\n",
    "        if softmax:\n",
    "            return torch.softmax(out,dim = 1)\n",
    "        else:\n",
    "            return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7929cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# #batch = dgl.load_graphs('./graph_decomp/chunk_3')\n",
    "# bl = BaseLine(54,300,18)\n",
    "# spin = Spin2(54,300,17)\n",
    "# graphs = batch[0]\n",
    "# last_action_nodes = batch[1]['last_action']\n",
    "# last_atom_feats = batch[1]['last_atom_feats']\n",
    "# actions = batch[1]['actions']\n",
    "# i = 64\n",
    "# # print(graphs[i],last_action_nodes[i],last_atom_feats[i])\n",
    "# # print(actions[i])\n",
    "\n",
    "# p = bl(graphs[i],torch.unsqueeze(last_action_nodes[i],dim=0),torch.unsqueeze(last_atom_feats[i],dim=0))\n",
    "# s = spin(graphs[i],torch.unsqueeze(last_action_nodes[i],dim=0),torch.unsqueeze(last_atom_feats[i],dim=0))\n",
    "# p.shape, s.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeea0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbeb80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spin2(torch.nn.Module):\n",
    "    '''\n",
    "    Sharing network for message passing, we can then bulk up the dense layers\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim, num_nodes):\n",
    "        super(Spin2, self).__init__()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.GGN = dgl.nn.GatedGraphConv(in_dim,hidden_dim,5,4)\n",
    "        \n",
    "        self.Weight = Linear_3(hidden_dim*2, hidden_dim*2, 3)\n",
    "        self.AddNode = Linear_3(hidden_dim, hidden_dim*2,num_nodes)\n",
    "        self.BatchEdge = Batch_Edge(in_dim,hidden_dim)\n",
    "        \n",
    "    def forward(self, graph, last_action_node, last_node,softmax = True):\n",
    "        out = []\n",
    "        h = F.relu(self.GGN(graph,graph.ndata['atomic'],graph.edata['type'].squeeze()))\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            hg = dgl.mean_nodes(graph, 'h')\n",
    "        \n",
    "                        \n",
    "        last_action_node = last_action_node.repeat(1,self.hidden_dim)\n",
    "        catted = torch.cat((hg,last_action_node),dim =1 )\n",
    "        weights = self.Weight(catted)\n",
    "        weights = torch.sigmoid(weights)\n",
    "        \n",
    "        addNode = self.AddNode(hg)\n",
    "        edges = self.BatchEdge(graph,last_node,h)\n",
    "        \n",
    "        addNode = addNode * weights[0][0]\n",
    "        edges = edges * weights[0][1]\n",
    "        terminate = weights[:,2:3]/2\n",
    "        \n",
    "        \n",
    "        out = torch.cat((terminate,addNode,edges),dim = 1)\n",
    "        if softmax:\n",
    "            return torch.softmax(torch.cat((terminate,addNode,edges),dim = 1),dim = 1)\n",
    "        else:\n",
    "            return torch.cat((terminate,addNode,edges),dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7401c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch_Edge(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(Batch_Edge, self).__init__()\n",
    "        self.NodeEmbed = nn.Linear(in_dim, hidden_dim)\n",
    "        self.Edges = Linear_3(hidden_dim*2,hidden_dim*2,2)\n",
    "        \n",
    "    def forward(self, graphs, last_node_batch, h):\n",
    "        '''\n",
    "        graph is actually a batch of graphs where len(graph.batch_num_nodes()) = len(last_node_batch)\n",
    "        returns just a list of edges so its unsplit up rn, bbut     \n",
    "        '''\n",
    "        edges_per_graph = []\n",
    "        \n",
    "        node_embed_batch = self.NodeEmbed(last_node_batch)\n",
    "        batch_node_stacks = [] #holds number of graphs of each node embedding stacked to match node number per graph\n",
    "                               #so its first dimension is the batch size, and the second 'dimension' is the number of nodes per grah\n",
    "        \n",
    "        \n",
    "        '''looping over graphs'''\n",
    "        for i in range(last_node_batch.shape[0]):\n",
    "            batch_node_stacks.append(node_embed_batch[i].repeat(graphs.batch_num_nodes()[i],1))\n",
    "        \n",
    "                                    \n",
    "        batch_node_stacks = torch.cat(batch_node_stacks, dim = 0)\n",
    "        \n",
    "        stack = torch.cat((h,batch_node_stacks),dim = 1)\n",
    "        edges = self.Edges(stack)\n",
    "        with graphs.local_scope():\n",
    "            graphs.ndata['bond_pred'] = edges\n",
    "            graphs = dgl.unbatch(graphs)\n",
    "            for graph in graphs:\n",
    "                edges_per_graph.append(graph.ndata['bond_pred'])\n",
    "                \n",
    "        return pad_sequence(edges_per_graph, batch_first = True, padding_value=-10000).flatten(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb2087",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.full((4,1),-10000)*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5db458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
